{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach to the solution\n",
    "\n",
    "##### 1. Which ML/DL model architecture you used and why?\n",
    "- I am using webscrapping by utilising the yahoo's search engine to get the most relevant articles for the given query.\n",
    "- I have used LDA i.e. <b>Latent Dirichlet Model</b> to train my model primarily because it is currently one of the most popular topic modelling technique.\n",
    "- There are other techniques as well such Term Frequency and Inverse Document Frequency, NonNegative Matrix Factorization techniques which I haven't used for now.\n",
    "\n",
    "##### 2. How would you ensure the scalability of your solution?\n",
    "- For the top 5 articles, we mostly deal with less dataset. Incase we start dealing with more and more articles, our dataset increases and hence according to me the Cloud solution is the best. We can use multi-GPU power of DataFlow pipeline to create our compressed dataset in the form of tfrecords and then deploy over AI Platform. This Multi-GPU approach makes this solution highly scalable.\n",
    "- And if we talk about the scalability of the number of articles then we have to make sure that we are able to navigate to the next pages(eg: yahoo's next pages in our case). Once we achieve this we become fairly confident that even on increasing the number of articles we don't get limited by the articles that we can access.\n",
    "\n",
    "##### 3. Is there a need for any dataset? If yes then how much data is sufficient to train the model in order to get the required results?\n",
    "- Yes. We are scrapping the top query relevant articles from the web. So, the result of this webscrapping would be the multiple web articles out of which we have to ensure few parameters as follows:\n",
    "    - articles text size\n",
    "    - number of unique words in each article\n",
    "    - total vocabulary size from all the articles\n",
    "    - Permission to scrap the website\n",
    "- Once we make sure that we satisfy the above parameters then we are good to select our top five required articles.\n",
    "- Dataset with unqiue words of more than 1000 should be suffiecient to get the required results.\n",
    "\n",
    "##### 4. Is there a need to create manual datasets, if yes then what parameters and sample size did you consider to create a dataset? \n",
    "- No.\n",
    "- If we use potentially strong search engine then we will always get enough number of articles.\n",
    "- Even if the articles on the first page are not sufficient, still we can move over to the next page to webscrap further articles.\n",
    "\n",
    "##### 5. Is your model and dataset generalized enough for different domains of the use cases, How?\n",
    "- Yes.\n",
    "- As discussed in the point two, I have considered the parameters sensitive for the possible test cases.\n",
    "- We can further improve it by getting accessible to the articles over the next pages. The reason I haven't implemented this feature is because we are dealing with only 5 articles so I didn't find the need for it in our usecase.\n",
    "- Also the model I have chosed is LDA which is currently the most popular in topic/keywords modelling.\n",
    "\n",
    "##### 6. How would you train, test and deploy your model to production? \n",
    "- One of the production effiecint technique which I know is by using GCP's DataFlow Pipeline and AI Platform's tf.estimator API both of which allows us the use of multiple GPUs.\n",
    "- Using DataFlow pipeline we can have even terabytes of data in our articles which will be converted into tfrecords and get stored on the cloud storage.\n",
    "- Once the tfrecords are obtained, we can use AI Platform which allows us to perform 3 operations on the dataset anytime.\n",
    "- i.e.    i) training ii) evaluation iii) prediction\n",
    "- On AI Platform, we can deploy our model and then our code will run in the prediction mode as per the tf estimator API.\n",
    "\n",
    "##### 7. How would you perform hyperparameter tuning on your model to improve accuracy?\n",
    "- I have provided the solution with the visualization, I have observed that the topics don't get overlap mostly when I set 3 topics.\n",
    "- Possible reason for this could be a less dataset that is getting scrapped becasue we are dealling with only top 5 articles.\n",
    "- Had been the case that we were using more articles, we would have got the freedom to get the results from even more topics.\n",
    "- Hyperparameters : num of topics, number of unqiue words required, article.text length\n",
    "- Even we can go for the other search engines as well which will just require more work right from getting the html elements.\n",
    "\n",
    "##### 8. Anything else you want to let us know about your approach.\n",
    "- The current approach looks fairly appealing to me because the differences in the results are direclty visible to me.\n",
    "- However there is certain overlaps in the topics as we go on increasing the number of topic becasue we are only using the dataset from 5 articles.\n",
    "- Still this LDA approach can be utilsed for even one topic which will atleast provide us with the most relevant keywords. However this model is fairly working amazing even with 3 topics and even sometimes upto 5 even for such smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <hr>\n",
    "### Model/Code on GitHub repo or Colab Notebook with the necessary documentation describing the model functioning.\n",
    "- Code is pushed on my github repo. It can be found on this url : https://github.com/vishalw-iitk/ML_new/blob/master/CareerNinja/vishal_w.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <hr>\n",
    "### Only Approach to generate different types of questions (short answer type, MCQs, true/false, fill in the blanks, long answer type, etc.) for that same article.\n",
    "- We can generate questions using <b>allennlp</b> for all the articles\n",
    "- Or this is one of the theoritical approach I know currently:\n",
    "    - Identify keywords from the text and use them as answers to the questions.\n",
    "    - Replace the answer from the sentence with blank space and use it as the base for the question.\n",
    "    - Transform the sentence with a blank space for answer to a more question-like sentence.\n",
    "    - Generate distractors, words that are similar to the answer, as incorrect answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # To use regex operations\n",
    "from bs4 import BeautifulSoup #To parse response text using the html parser\n",
    "import requests #To make request from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True) #stopwords\n",
    "nltk.download('wordnet', quiet=True) #\n",
    "from newspaper import Article #To read the content of a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim #Gensim is an open-source library for unsupervised topic modeling and natural language processing\n",
    "from gensim.models.ldamulticore import LdaMulticore # for multipreprocessing\n",
    "from gensim import corpora, models #\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim #for visualization\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vishal waghmare\\3d objects\\vscode_codes\\github\\pandas\\careerninja\\careerninja\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using Yahoo Search Engine for WebScrapping\n",
    "template = 'https://in.search.yahoo.com/search;_ylt=AwrwIQa9FFJgmlQA7hq7HAx.;_ylu=Y29sbwNzZzMEcG9zAzEEdnRpZAMEc2VjA3BpdnM-?p={}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your main query(eg: Earphones in the market) : Earphones in the market\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your main query(eg: Earphones in the market) : \")\n",
    "url = template.format(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the get request, we first set the request headers\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'referer': 'https://www.google.com',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cards = soup.find_all('div', 'NewsArticle')\n",
    "cards = soup.find_all('div', 'algo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the articles result currently in the html form\n",
    "len(cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the headline of the article and its url\n",
    "def get_article(card):\n",
    "    '''Extract article information from the raw html'''\n",
    "#     headline = card.find('h4', 's-title').text\n",
    "    headline = card.find('h3', 'title').text\n",
    "    raw_link = card.find('a').get('href')\n",
    "    unquoted_link = requests.utils.unquote(raw_link)\n",
    "    pattern = re.compile(r'RU=(.+)\\/RK')\n",
    "    clean_link = re.search(pattern, unquoted_link).group(1)\n",
    "    article = [headline, clean_link]\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the headline and url for every article on the first page in the articles list\n",
    "articles = []\n",
    "links = set()\n",
    "for card in cards:\n",
    "    article = get_article(card)\n",
    "    link = article[-1]\n",
    "    if not link in links:\n",
    "        links.add(link)\n",
    "        articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class top_article:\n",
    "    def __init__(self, article_name, article_link):\n",
    "        '''To set article name and article link for every article'''\n",
    "        self.article_name = article_name\n",
    "        self.article_link = article_link\n",
    "\n",
    "    def get_uncleaned_webcontent(self):\n",
    "        '''To get the cleaned web article from all the web articles'''\n",
    "        \n",
    "        article = Article(self.article_link)\n",
    "        \n",
    "        try :\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            nltk.download('punkt', quiet = True)\n",
    "            article.nlp()\n",
    "            \n",
    "            if len(article.text) > 100:\n",
    "                '''Taking only the articles with total text length greater than 100'''\n",
    "                self.article_content = article.text\n",
    "            else:\n",
    "                self.article_content = \"It is invalid. Go for next\"\n",
    "            \n",
    "        except:\n",
    "            '''Exception if the article does not have permission to get scrapped'''\n",
    "            self.article_content = \"It is invalid. Go for next\"\n",
    "        \n",
    "        \n",
    "    def clean(self):\n",
    "        '''\n",
    "        Splitting the data\n",
    "        Removing the punctuations\n",
    "        Lemmatization : making sure that words like spectacle/spectacles are considered same\n",
    "        '''\n",
    "        stop_free = ' '.join([word for word in self.article_content.lower().split() if word not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = ' '.join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "        self.article_cleaned =  normalized.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(bag_of_words):\n",
    "    '''Make a single dictionary of the words from top 5 articles'''\n",
    "    dictionary = corpora.Dictionary(bag_of_words)\n",
    "    return dictionary\n",
    "\n",
    "def make_doc_term_matrix(dictionary, bag_of_words):\n",
    "    '''\n",
    "    A matrix which stores the occurence and frequency of each word from the bag of words\n",
    "    As we have 5 articles, this matrix will have 5 rows\n",
    "    '''\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in bag_of_words]\n",
    "    return doc_term_matrix\n",
    "\n",
    "def build_lda_model(dictionary, doc_term_matrix, lda, num_topics):\n",
    "    '''Training the model by passing the hyperparameters'''\n",
    "    lda_model = lda(doc_term_matrix, num_topics = num_topics, id2word=dictionary, passes=50, minimum_probability=0)\n",
    "    return lda_model\n",
    "\n",
    "def print_topic_clusters(lda_model, num_topics):\n",
    "    '''\n",
    "    Print the clusters of selected number of topics\n",
    "    Each cluster shows the relevant keywords associated with that cluster using which we can determine the topic of that cluster\n",
    "    '''\n",
    "    print('\\033[1m')\n",
    "    print(\"\\nKeywords/Topics/Tags for the articles\\n\")\n",
    "    print('\\033[0m')\n",
    "    keywords_and_probs = lda_model.print_topics(num_topics=num_topics)\n",
    "    keywords_only = []\n",
    "    for i in keywords_and_probs:\n",
    "        '''Original output contains the probability along with the keywords. Here we are trying to extract only the keywords'''\n",
    "        keywords_only.append(re.findall(r'\"(.*?)\"', i[1]))\n",
    "    for i, cluster_keywords in enumerate(keywords_only):\n",
    "        '''We print the cluster number and the keywords associated with that cluster'''\n",
    "        print(\"Cluster\",i+1,cluster_keywords)\n",
    "\n",
    "def build_lda_display(lda_model, doc_term_matrix, dictionary):\n",
    "    '''Here we build the lda_display object which helps us to visualize our results'''\n",
    "    lda_display = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary, sort_topics = False, mds='mmds')\n",
    "    return lda_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of topics we want to get(recommended less than 5)(say 3) :  3\n",
      "\n",
      "Please wait...\n",
      "\n",
      "Top 5 articles are :\n",
      "1. https://www.grandviewresearch.com/industry-analysis/earphone-and-headphone-market\n",
      "2. https://www.gminsights.com/industry-analysis/earphone-and-headphone-market\n",
      "3. https://www.transparencymarketresearch.com/earphone-market.html\n",
      "4. https://www.marketwatch.com/press-release/global-hi-fi-earphones-market-2020-applications-market-size-according-to-a-specific-product-sales-and-revenue-by-region-2025-2021-03-17\n",
      "5. https://www.mordorintelligence.com/industry-reports/earphones-and-headphones-market\n",
      "\n",
      "Building the clusters of topics....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:sklearn not present, switch to PCoA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "\n",
      "Keywords/Topics/Tags for the articles\n",
      "\n",
      "\u001b[0m\n",
      "Cluster 1 ['earphone', 'headphone', 'market', 'wireless', 'corporation', 'inc', 'audio', 'technology', '2020', 'consumer']\n",
      "Cluster 2 ['market', 'report', 'earphone', 'research', 'business', 'study', 'global', 'customer', 'key', 'analysis']\n",
      "Cluster 3 ['done', 'nicca', 'brian', 'the', 'vp', 'excellent', 'moore', 'usa', 'inc', 'quality']\n"
     ]
    }
   ],
   "source": [
    "top_articles = []\n",
    "for article in articles:\n",
    "    '''Getting all articles obtained from the search engine. This articles are obtained on the 1st page'''\n",
    "    top_articles.append(top_article(article[0], article[1]))\n",
    "\n",
    "# Initializing the model object\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "\n",
    "# User input for the number of topics would we like to achieve\n",
    "num_topics = int(input(\"Enter the number of topics we want to get(recommended less than 5)(say 3) :  \"))\n",
    "print(\"\\nPlease wait...\")\n",
    "\n",
    "\n",
    "count = 0\n",
    "valid_top_articles = []\n",
    "for num, article in enumerate(top_articles):\n",
    "    if count >= 5:\n",
    "        # Inorder to get the top 5 articles\n",
    "        break\n",
    "    article.get_uncleaned_webcontent() #Getting the data for every top 5 article\n",
    "    if article.article_content == \"It is invalid. Go for next\":\n",
    "        continue\n",
    "    count = count + 1\n",
    "    article.clean() #Cleaning the the data of every top 5 articles\n",
    "    valid_top_articles.append(article)\n",
    "\n",
    "print(\"\\nTop 5 articles are :\")\n",
    "bag_of_words = []\n",
    "for num, article in enumerate(valid_top_articles):\n",
    "    '''Printing the url for top 5 articles and creating a bag of words from all of them'''\n",
    "    print(str(num+1)+\".\", article.article_link)\n",
    "    bag_of_words.append(article.article_cleaned)\n",
    "\n",
    "dictionary = make_dictionary(bag_of_words) #Making a dcitionary from the bag of words\n",
    "doc_term_matrix = make_doc_term_matrix(dictionary, bag_of_words) #creating a matrix\n",
    "\n",
    "print(\"\\nBuilding the clusters of topics....\")\n",
    "lda_model = build_lda_model(dictionary, doc_term_matrix, lda, num_topics) #training the model\n",
    "\n",
    "print_topic_clusters(lda_model, num_topics) #printing the cluster results\n",
    "lda_display = build_lda_display(lda_model, doc_term_matrix, dictionary) #building the object capable to show visualization of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1542818873116819606271969677\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1542818873116819606271969677_data = {\"mdsDat\": {\"x\": [-0.11216277850580675, 0.09184023596823152, 0.020322542537575233], \"y\": [-0.013923885685146112, -0.02579376342662069, 0.03971764911176676], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [41.577585627556985, 58.36271260885475, 0.059701763588268196]}, \"tinfo\": {\"Term\": [\"headphone\", \"wireless\", \"inc\", \"business\", \"corporation\", \"research\", \"study\", \"analysis\", \"2026\", \"2020\", \"audio\", \"technology\", \"framework\", \"among\", \"strategic\", \"type\", \"service\", \"hifi\", \"volume\", \"insight\", \"help\", \"earphone\", \"noise\", \"adoption\", \"2019\", \"increased\", \"developing\", \"smart\", \"cancellation\", \"rise\", \"headphone\", \"2026\", \"among\", \"smart\", \"rise\", \"increased\", \"developing\", \"connectivity\", \"cancellation\", \"2019\", \"enthusiast\", \"shipment\", \"group\", \"fitness\", \"pdf\", \"enhancing\", \"instance\", \"base\", \"usd\", \"lg\", \"popularity\", \"billion\", \"active\", \"request\", \"cagr\", \"sport\", \"improved\", \"advanced\", \"ai\", \"june\", \"inc\", \"wireless\", \"2020\", \"corporation\", \"audio\", \"technology\", \"noise\", \"adoption\", \"earphone\", \"experience\", \"feature\", \"increasing\", \"international\", \"consumer\", \"industry\", \"market\", \"device\", \"player\", \"bluetooth\", \"growth\", \"report\", \"u\", \"demand\", \"business\", \"study\", \"analysis\", \"framework\", \"strategic\", \"service\", \"type\", \"volume\", \"help\", \"insight\", \"hifi\", \"assessment\", \"recent\", \"analyst\", \"research\", \"current\", \"regional\", \"including\", \"opportunity\", \"via\", \"tmr\", \"portable\", \"dynamic\", \"map\", \"journey\", \"result\", \"boost\", \"datadriven\", \"cxos\", \"stakeholder\", \"global\", \"report\", \"market\", \"also\", \"customer\", \"key\", \"various\", \"value\", \"segment\", \"new\", \"america\", \"earphone\", \"product\", \"africa\", \"offer\", \"growth\", \"industry\", \"player\", \"size\", \"demand\", \"augmenting\", \"rapid\", \"pandemic\", \"helped\", \"earbuds\", \"available\", \"bone\", \"buying\", \"highquality\", \"personalized\", \"3d\", \"immersive\", \"singapore\", \"format\", \"uptake\", \"operating\", \"calorie\", \"innovative\", \"spending\", \"credited\", \"atmos\", \"function\", \"versatility\", \"box\", \"component\", \"frequently\", \"assistance\", \"estimate\", \"expand\", \"assistant\", \"done\", \"excellent\", \"moore\", \"usa\", \"vp\", \"brian\", \"nicca\", \"the\", \"entertainment\", \"click\", \"shape\", \"connected\", \"kingdom\", \"here\", \"vertical\", \"per\", \"enduser\", \"defense\", \"avionics\", \"telecommuication\", \"middleeast\", \"automotive\", \"reflects\", \"inc\", \"quality\", \"u\", \"impacted\", \"register\", \"wired\", \"rest\"], \"Freq\": [24.0, 18.0, 15.0, 13.0, 19.0, 18.0, 11.0, 9.0, 7.0, 11.0, 12.0, 12.0, 8.0, 6.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0, 6.0, 46.0, 9.0, 9.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 23.813297539924143, 7.081964988806855, 6.3371478007688955, 4.816196842372099, 4.816196301423741, 4.816195760475384, 4.816196301423741, 4.816196301423741, 4.816195760475384, 4.816195219527026, 4.816195760475384, 4.055720281276986, 4.0557194698544485, 4.055720281276986, 4.055718928906091, 3.2952439906560507, 3.295243179233514, 3.295243179233514, 3.295243179233514, 3.2952429087593353, 3.2952429087593353, 3.2952429087593353, 3.2952426382851563, 3.2952429087593353, 3.2314491394162497, 3.231428853852838, 2.534765806715864, 2.534765806715864, 2.534765806715864, 2.5347652657675064, 14.246847623770497, 16.031449238407244, 9.380309586287447, 15.4641393853958, 10.140269541701008, 10.140269541701008, 7.098151786512714, 7.098151245564357, 25.235703936008456, 6.337610311614683, 4.816577670015882, 4.816577129067524, 4.816577129067524, 9.379880073291476, 9.379703724126884, 19.53208942714246, 5.5770309703316165, 7.098046301582973, 4.816526820870263, 6.490402257128683, 7.3355253353316225, 4.863792724567993, 4.816362372569539, 13.320686687933158, 11.689556114785058, 9.242856078733958, 8.427292690491246, 7.611727024250928, 6.796157561347929, 6.796155283350322, 5.98059948843297, 5.98059948843297, 5.980598349434167, 5.980589996776271, 5.165033442526384, 5.165031544195044, 5.165031544195044, 17.397281863096357, 4.349466637287261, 4.3494677762860645, 4.349463599957117, 4.349462460958313, 4.349462460958313, 3.5339017303794784, 3.53390135071321, 3.5339017303794784, 3.53390135071321, 3.5339009710469425, 3.5339009710469425, 3.5339009710469425, 3.5339009710469425, 3.5339009710469425, 3.53390135071321, 10.874218248305526, 26.93015665128137, 47.28826286787968, 6.796364099797709, 9.243081600497137, 9.243065654513881, 8.427501506938635, 5.9807616059293975, 9.181170940842918, 5.1651815123708955, 7.367546182665482, 20.782330117787023, 8.427130193328551, 5.915871325404081, 5.165173539379268, 8.26293971907847, 8.426607013211276, 7.6112881300451445, 5.884815763682225, 5.980430916609988, 0.0015163225781741308, 0.0015163224810798355, 0.0015163224810798355, 0.0015163222868912454, 0.0015163222868912454, 0.0015163222868912454, 0.0015163221897969501, 0.0015163221897969501, 0.0015163221897969501, 0.0015163221897969501, 0.0015163221897969501, 0.0015163221897969501, 0.0015163221897969501, 0.0015163219956083598, 0.0015163219956083598, 0.0015163219956083598, 0.0015163219956083598, 0.0015163219956083598, 0.0015163219956083598, 0.0015163219956083598, 0.0015163218985140647, 0.0015163218985140647, 0.0015163218985140647, 0.0015163218985140647, 0.0015163218985140647, 0.0015163218985140647, 0.0015163218985140647, 0.0015163218985140647, 0.0015163218985140647, 0.0015163218985140647, 0.0015216528607888313, 0.0015216528607888313, 0.0015216528607888313, 0.0015216528607888313, 0.0015216528607888313, 0.0015216528607888313, 0.0015216528607888313, 0.0015216528607888313, 0.0015171010802325787, 0.0015171008860439884, 0.001517100691855398, 0.001517100594761103, 0.001517100594761103, 0.0015171004976668077, 0.0015171003034782173, 0.0015171002063839223, 0.0015171002063839223, 0.0015171002063839223, 0.0015171002063839223, 0.001517100012195332, 0.001517100012195332, 0.0015170999151010369, 0.0015170999151010369, 0.0015182943691198726, 0.0015179437616201152, 0.001517326533185898, 0.0015171313736526628, 0.0015171313736526628, 0.001517039134072278, 0.001517039036977983], \"Total\": [24.0, 18.0, 15.0, 13.0, 19.0, 18.0, 11.0, 9.0, 7.0, 11.0, 12.0, 12.0, 8.0, 6.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0, 6.0, 46.0, 9.0, 9.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 24.102441571636927, 7.372015514978687, 6.6105246132515685, 5.089572109450635, 5.089571924051026, 5.089571385918403, 5.089572067688102, 5.089572278337549, 5.0895717160874065, 5.08957132023254, 5.089571999380693, 4.329095264285481, 4.329094997953544, 4.329096019831339, 4.329094623303367, 3.5686181159200476, 3.5686176156949942, 3.5686180627620097, 3.568618181116436, 3.5686179129725204, 3.568617935827813, 3.5686179849367, 3.5686178331082297, 3.5686184809184134, 3.572976494220947, 3.5729782772448333, 2.808140001128306, 2.808140071733166, 2.8081401657758844, 2.8081396280316384, 15.823763381479782, 18.14151466926227, 11.283241027808662, 19.813760224933446, 12.859332308897773, 12.859332873833745, 9.001872503521835, 9.001874615285544, 46.019550652121445, 8.24140221109724, 5.904876698730631, 5.904876820741828, 5.904877576384781, 14.545547608269237, 17.807827173613752, 66.82186895694487, 8.296503575807538, 14.710850852854096, 6.720458810409104, 14.754858447922164, 34.2671986047462, 9.16357299715749, 10.798309693802379, 13.575702304537199, 11.944571438573004, 9.497871442448115, 8.682306432147314, 7.866741205330442, 7.051171993073929, 7.051170582358765, 6.235612453845324, 6.235612854216406, 6.235612207782314, 6.235605278200416, 5.42004602856048, 5.420044745107172, 5.420044795432704, 18.413718287545056, 4.6044788667704895, 4.604480334832247, 4.604476907054683, 4.604475859278566, 4.604475993544713, 3.7889138480458002, 3.788913452834216, 3.7889139479201086, 3.788913588556777, 3.7889132608666434, 3.7889132773829024, 3.7889132937049728, 3.7889133279026224, 3.7889133459723903, 3.7889137898589023, 12.649814188447849, 34.2671986047462, 66.82186895694487, 7.811500559150326, 11.018686572784539, 11.018683488015071, 10.203121347344139, 6.995932409094501, 11.774974420964845, 6.180368593642829, 10.131490782787603, 46.019550652121445, 12.48452459858961, 7.752021206446622, 6.180366443613589, 14.754858447922164, 17.807827173613752, 14.710850852854096, 8.510402689413818, 10.798309693802379, 1.2871754465821215, 1.2871754227558856, 1.2871754939433109, 1.287175422561697, 1.2871754462908387, 1.2871754664511, 1.2871754224646026, 1.2871754224646026, 1.2871754224646026, 1.2871754426248638, 1.2871754663540056, 1.2871754936520279, 1.2871754936520279, 1.287175466159817, 1.287175466159817, 1.2871754697286975, 1.2871754898889585, 1.2871755171869808, 1.287175530209481, 1.2871755409161225, 1.2871754696316033, 1.2871754696316033, 1.2871754897918644, 1.287175493360745, 1.287175506383245, 1.2871755135210061, 1.2871755170898866, 1.2871755170898866, 1.2871755170898866, 1.2871755372501479, 1.3410747154641607, 1.3410747154641607, 1.3410747154641607, 1.3410747154641607, 1.3410747154641607, 1.341074749273433, 1.341074749273433, 1.341074749273433, 1.3410288077412142, 1.341028824451662, 1.3410288749713817, 1.3410289021723099, 1.3410289021723099, 1.3410289696937603, 1.3410289187856632, 1.341028935593205, 1.3410289901892494, 1.341029040903158, 1.3410290512965442, 1.3410290407089696, 1.3410290680069918, 1.3410290406118752, 1.3410290406118752, 15.823763381479782, 6.010935141538738, 9.16357299715749, 2.077861778693831, 2.077862235130169, 2.818621946473846, 2.97228189496154], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.8874, -5.1001, -5.2112, -5.4856, -5.4856, -5.4856, -5.4856, -5.4856, -5.4856, -5.4856, -5.4856, -5.6575, -5.6575, -5.6575, -5.6575, -5.8651, -5.8651, -5.8651, -5.8651, -5.8651, -5.8651, -5.8651, -5.8651, -5.8651, -5.8847, -5.8847, -6.1275, -6.1275, -6.1275, -6.1275, -4.4011, -4.2831, -4.819, -4.3191, -4.7411, -4.7411, -5.0978, -5.0978, -3.8294, -5.2111, -5.4856, -5.4856, -5.4856, -4.8191, -4.8191, -4.0856, -5.339, -5.0978, -5.4856, -5.1873, -5.0649, -5.4758, -5.4856, -4.8074, -4.938, -5.1729, -5.2653, -5.367, -5.4804, -5.4804, -5.6082, -5.6082, -5.6082, -5.6082, -5.7548, -5.7548, -5.7548, -4.5404, -5.9267, -5.9267, -5.9267, -5.9267, -5.9267, -6.1343, -6.1343, -6.1343, -6.1343, -6.1343, -6.1343, -6.1343, -6.1343, -6.1343, -6.1343, -5.0103, -4.1035, -3.5405, -5.4803, -5.1729, -5.1729, -5.2652, -5.6082, -5.1796, -5.7548, -5.3997, -4.3626, -5.2653, -5.6191, -5.7548, -5.285, -5.2653, -5.3671, -5.6244, -5.6082, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -7.0031, -6.9996, -6.9996, -6.9996, -6.9996, -6.9996, -6.9996, -6.9996, -6.9996, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0026, -7.0018, -7.0021, -7.0025, -7.0026, -7.0026, -7.0027, -7.0027], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8655, 0.8375, 0.8354, 0.8224, 0.8224, 0.8224, 0.8224, 0.8224, 0.8224, 0.8224, 0.8224, 0.8124, 0.8124, 0.8124, 0.8124, 0.7979, 0.7979, 0.7979, 0.7979, 0.7979, 0.7979, 0.7979, 0.7979, 0.7979, 0.7771, 0.7771, 0.7752, 0.7752, 0.7752, 0.7752, 0.7726, 0.754, 0.6929, 0.6298, 0.6401, 0.6401, 0.64, 0.64, 0.2768, 0.6149, 0.6739, 0.6739, 0.6739, 0.4389, 0.2365, -0.3524, 0.4804, 0.1488, 0.5445, 0.0564, -0.6639, 0.2442, 0.0702, 0.5195, 0.5169, 0.5113, 0.5087, 0.5055, 0.5017, 0.5017, 0.4967, 0.4967, 0.4967, 0.4967, 0.4903, 0.4903, 0.4903, 0.4817, 0.4815, 0.4815, 0.4815, 0.4815, 0.4815, 0.4688, 0.4688, 0.4688, 0.4688, 0.4688, 0.4688, 0.4688, 0.4688, 0.4688, 0.4688, 0.3872, 0.2976, 0.1927, 0.3993, 0.3628, 0.3628, 0.3473, 0.3817, 0.2897, 0.3591, 0.2199, -0.2565, 0.1455, 0.2682, 0.3591, -0.0413, -0.2098, -0.1205, 0.1696, -0.0524, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6796, 0.6421, 0.6421, 0.6421, 0.6421, 0.6421, 0.6421, 0.6421, 0.6421, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, 0.6392, -1.8281, -0.8604, -1.2825, 0.2013, 0.2013, -0.1037, -0.1568]}, \"token.table\": {\"Topic\": [1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2], \"Freq\": [0.9824010089265341, 0.7976431574774137, 0.17725403499498082, 0.9495367970641388, 0.7768948571033243, 0.8406616063415876, 0.7776158077245066, 0.22217594506414473, 1.0683227771285708, 0.2579972302367787, 0.7739916907103361, 1.0683227413511622, 0.12801637693395648, 0.8961146385376955, 0.29610647281017144, 0.6909151032237334, 0.9076435458650739, 0.9475807347504183, 0.9225016007641372, 0.9225013908835676, 0.776894826480892, 0.7768948143128527, 0.776894855125079, 0.7776453520126149, 0.23329360560378445, 0.7768948690369539, 0.7456960063621941, 0.7768948570447215, 0.74569600042085, 0.8406615522419019, 0.8406615705752584, 0.7439968223978487, 0.29759872895913947, 0.7768948835934599, 1.0557116750720408, 0.7768948408029852, 0.7456705903543257, 0.9575931843802445, 0.7768948835934599, 0.8396360862861263, 0.7768948428984361, 0.9824009325177041, 0.7456961265608094, 0.7768948329430524, 0.7456960833432575, 0.9824008239908901, 0.618746041220438, 0.3437478006780211, 0.7570496377120858, 0.20187990338988954, 0.7768948121001966, 0.8687193742742788, 0.18150983665692733, 0.816794264956173, 1.0557116605086772, 1.0557116655434886, 0.7456960062002227, 0.4630354325612386, 0.5556425190734863, 0.9824008646509275, 0.7231962169576949, 0.36159810847884744, 0.7456706091531142, 1.055711492786941, 0.7768948692127623, 0.5432473730346502, 0.45632779334910617, 0.7456960344003283, 0.8406615397194304, 0.7456961358528664, 0.9824008778357802, 0.776894826480892, 0.7456706091531142, 0.776894826480892, 0.72803144978422, 0.24267714992807332, 0.8467577318040946, 0.16935154636081895, 0.923980429557633, 0.7768948572205299, 0.9214141498598817, 0.7768948286349455, 0.776894855125079, 0.15810508915035715, 0.8695779903269644, 0.9239806476621293, 0.4066457174887329, 0.5421942899849772, 0.9957497429738621, 0.9622149643146802, 0.7768948835348571, 0.7456960457971029, 0.9622161333681449, 0.7768948835934599, 0.776894840627177, 0.4812639658007531, 0.4812639658007531, 1.0683228039893327, 0.8847452823002697, 0.12639218318575282, 0.86871974401076, 0.9824009962476947, 0.8467577143077223, 0.16935154286154444, 0.5053957404379742, 0.4492406581670882, 0.7768948264222894, 0.9622150640656808, 0.8406616575577669, 0.8467576059487443, 0.16935152118974883, 1.0557116842218432, 1.0683229459294536, 0.1815098874720726, 0.8167944936243267, 0.7456960833432575, 0.8406615875279055, 1.0557115929169625, 0.29930321184052094, 0.7033625478252242, 0.7456959911287965, 0.7456706091531142, 0.16180264734187652, 0.8090132367093826, 0.7456705903543257, 0.7776159901466462, 0.22217599718475606, 0.16180270362986948, 0.8090135181493474, 0.7768948550664764, 0.8687199416931516, 0.7768948404513686, 0.923980727625619, 0.7456960647591465, 0.7768948714254187, 0.47583923391092703, 0.5438162673267737, 0.8406615821438698, 1.0557116307335777, 0.32039666135560213, 0.6407933227112043, 0.4990903959798892, 0.4990903959798892, 0.7768948834176514, 0.9225016093296354, 0.7456960063621941, 0.8687190972975956, 0.4812638600832717, 0.4812638600832717, 0.20427698455135637, 0.7879255118409461, 0.8406614537365522, 0.05430733675753011, 0.9232247248780119, 1.0093255303561368, 1.0557116796198884, 0.9824008923760857, 0.25477762352151073, 0.7643328705645321, 0.9927427677095109, 0.745696098468678, 0.92398059081756, 0.776894840627177, 0.3525097588780062, 0.7050195177560123, 0.9824008565898277, 0.7768948185623568, 0.839635667282404, 1.0557115368278038, 1.0169395167822812, 1.0046404813862135, 0.7776453178491137, 0.23329359535473412, 0.7456960063082037, 0.7456705903543257, 1.055711520615089, 0.9927429663257916, 0.5456386937225233, 0.4365109549780186, 0.7768948572205299, 0.7456706091531142, 0.8406615243610779, 0.14294020318149875, 0.8576412190889925, 0.19601844689621356, 0.7840737875848542, 0.7768948429570388, 0.7456960741051925, 0.8687199163613485, 0.9622150260957882, 0.7456706091531142, 0.7095666031061886, 0.8819550236954191, 0.11024437796192739], \"Term\": [\"2019\", \"2020\", \"2020\", \"2026\", \"3d\", \"active\", \"adoption\", \"adoption\", \"advanced\", \"africa\", \"africa\", \"ai\", \"also\", \"also\", \"america\", \"america\", \"among\", \"analysis\", \"analyst\", \"assessment\", \"assistance\", \"assistant\", \"atmos\", \"audio\", \"audio\", \"augmenting\", \"automotive\", \"available\", \"avionics\", \"base\", \"billion\", \"bluetooth\", \"bluetooth\", \"bone\", \"boost\", \"box\", \"brian\", \"business\", \"buying\", \"cagr\", \"calorie\", \"cancellation\", \"click\", \"component\", \"connected\", \"connectivity\", \"consumer\", \"consumer\", \"corporation\", \"corporation\", \"credited\", \"current\", \"customer\", \"customer\", \"cxos\", \"datadriven\", \"defense\", \"demand\", \"demand\", \"developing\", \"device\", \"device\", \"done\", \"dynamic\", \"earbuds\", \"earphone\", \"earphone\", \"enduser\", \"enhancing\", \"entertainment\", \"enthusiast\", \"estimate\", \"excellent\", \"expand\", \"experience\", \"experience\", \"feature\", \"feature\", \"fitness\", \"format\", \"framework\", \"frequently\", \"function\", \"global\", \"global\", \"group\", \"growth\", \"growth\", \"headphone\", \"help\", \"helped\", \"here\", \"hifi\", \"highquality\", \"immersive\", \"impacted\", \"impacted\", \"improved\", \"inc\", \"inc\", \"including\", \"increased\", \"increasing\", \"increasing\", \"industry\", \"industry\", \"innovative\", \"insight\", \"instance\", \"international\", \"international\", \"journey\", \"june\", \"key\", \"key\", \"kingdom\", \"lg\", \"map\", \"market\", \"market\", \"middleeast\", \"moore\", \"new\", \"new\", \"nicca\", \"noise\", \"noise\", \"offer\", \"offer\", \"operating\", \"opportunity\", \"pandemic\", \"pdf\", \"per\", \"personalized\", \"player\", \"player\", \"popularity\", \"portable\", \"product\", \"product\", \"quality\", \"quality\", \"rapid\", \"recent\", \"reflects\", \"regional\", \"register\", \"register\", \"report\", \"report\", \"request\", \"research\", \"research\", \"rest\", \"result\", \"rise\", \"segment\", \"segment\", \"service\", \"shape\", \"shipment\", \"singapore\", \"size\", \"size\", \"smart\", \"spending\", \"sport\", \"stakeholder\", \"strategic\", \"study\", \"technology\", \"technology\", \"telecommuication\", \"the\", \"tmr\", \"type\", \"u\", \"u\", \"uptake\", \"usa\", \"usd\", \"value\", \"value\", \"various\", \"various\", \"versatility\", \"vertical\", \"via\", \"volume\", \"vp\", \"wired\", \"wireless\", \"wireless\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1542818873116819606271969677\", ldavis_el1542818873116819606271969677_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1542818873116819606271969677\", ldavis_el1542818873116819606271969677_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1542818873116819606271969677\", ldavis_el1542818873116819606271969677_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(lda_display) #Visualization of the results(works on local jupyter notebook).   It may not be visible over github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1b771199f60>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning the topics to the documents in the corpus\n",
    "lda_corpus = lda_model[doc_term_matrix]\n",
    "lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For article 1\n",
      "probability score for: \n",
      "\n",
      "topic 1 : 0.06905838\n",
      "topic 2 : 0.9044903\n",
      "topic 3 : 0.026451332\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "For article 2\n",
      "probability score for: \n",
      "\n",
      "topic 1 : 0.99939144\n",
      "topic 2 : 0.00031406892\n",
      "topic 3 : 0.00029452212\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "For article 3\n",
      "probability score for: \n",
      "\n",
      "topic 1 : 0.0003692049\n",
      "topic 2 : 0.9992921\n",
      "topic 3 : 0.00033874266\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "For article 4\n",
      "probability score for: \n",
      "\n",
      "topic 1 : 0.00065642956\n",
      "topic 2 : 0.99872684\n",
      "topic 3 : 0.0006167119\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "For article 5\n",
      "probability score for: \n",
      "\n",
      "topic 1 : 0.1932703\n",
      "topic 2 : 0.8028708\n",
      "topic 3 : 0.003858884\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Printing the probability score for topics for our top 5 articles\n",
    "for i, article in enumerate(lda_corpus):\n",
    "    print(\"\\n\\nFor article\",i+1)\n",
    "    print(\"probability score for: \\n\")\n",
    "    for j, topic in enumerate(article):\n",
    "        print(\"topic\",j+1,\":\",topic[1])\n",
    "    print(\"-----------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
